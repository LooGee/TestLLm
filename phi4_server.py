"""
Phi-4 ëª¨ë¸ ì„œë²„ ëª¨ë“œ
ëª¨ë¸ì„ í•œ ë²ˆ ë¡œë“œí•œ í›„ ê³„ì† ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import sys
import os
import time
from datetime import datetime
from device_utils import check_and_setup_device, get_device_info

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
_model = None
_tokenizer = None

# ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ì— ì‘ì„±í•  í”„ë¡¬í”„íŠ¸
# ì´ ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì •í•˜ì„¸ìš”
SCRIPT_PROMPT = """
ë‹¹ì‹ ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.
ì˜ˆ: "Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”"
"""

def load_quantized_model(model_dir=None):
    """4BIT ì–‘ìí™”ëœ phi-4 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        model_dir: ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ë˜ëŠ” Hugging Faceì—ì„œ ë¡œë“œ)
    """
    global _model, _tokenizer
    
    if _model is not None and _tokenizer is not None:
        print("ëª¨ë¸ì´ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return _model, _tokenizer
    
    # ê¸°ë³¸ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
    default_local_dir = "./models/phi4-quantized"
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²°ì •
    if model_dir is None:
        # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
        import sys
        if len(sys.argv) > 1 and "--model-dir" in sys.argv:
            idx = sys.argv.index("--model-dir")
            if idx + 1 < len(sys.argv):
                model_dir = sys.argv[idx + 1]
        # ê¸°ë³¸ ë¡œì»¬ ê²½ë¡œ í™•ì¸
        elif os.path.exists(default_local_dir) and os.path.isdir(default_local_dir):
            model_dir = default_local_dir
    
    model_name = "microsoft/phi-4"
    
    # GPU/CPU í™•ì¸ ë° ì„¤ì •
    device, device_name, is_gpu = check_and_setup_device()
    
    # 4BIT ì–‘ìí™” ì„¤ì •
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )
    
    start_time = time.time()
    
    # ë¡œì»¬ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    if model_dir and os.path.exists(model_dir):
        print(f"ë¡œì»¬ ì €ì¥ëœ ëª¨ë¸ ë¡œë”© ì¤‘: {model_dir}")
        print("(ì–‘ìí™”ëœ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ë¡œë“œí•©ë‹ˆë‹¤)")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir,
            trust_remote_code=True
        )
        
        # pad_token ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
        # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ cuda ì‚¬ìš©
        if torch.cuda.is_available():
            print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print("GPUì— ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            quantization_config=quantization_config,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        
        elapsed_time = time.time() - start_time
        print(f"ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
        
        # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
        device_info = get_device_info(model)
        print(f"\nëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device_info['device_name']} ({device_info['device']})")
        if device_info['is_gpu']:
            print("[OK] GPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
        else:
            print("[WARNING] CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (GPU ì‚¬ìš© ê¶Œì¥)")
        
        # ì „ì—­ ë³€ìˆ˜ì— ì €ì¥
        _model = model
        _tokenizer = tokenizer
        
        return model, tokenizer
    
    # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ Hugging Faceì—ì„œ ë¡œë“œ
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    print("4BIT ì–‘ìí™” ì„¤ì • ì ìš© ì¤‘...")
    print("(ì²˜ìŒ ë¡œë”© ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print("(ë¡œì»¬ ì €ì¥ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´: python save_quantized_model.py)")
    
    # Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ì„ íƒì‚¬í•­)
    cache_dir = os.getenv("HF_HOME", os.path.join(os.path.expanduser("~"), ".cache", "huggingface"))
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, 
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    
    # pad_token ì„¤ì • (ì—†ëŠ” ê²½ìš° eos_token ì‚¬ìš©)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ (ìµœì í™” ì˜µì…˜ ì¶”ê°€)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        cache_dir=cache_dir
    )
    
    elapsed_time = time.time() - start_time
    print(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
    
    # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
    device_info = get_device_info(model)
    print(f"\nëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device_info['device_name']} ({device_info['device']})")
    if device_info['is_gpu']:
        print("âœ… GPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
    else:
        print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (GPU ì‚¬ìš© ê¶Œì¥)")
    
    # ì „ì—­ ë³€ìˆ˜ì— ì €ì¥
    _model = model
    _tokenizer = tokenizer
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_new_tokens=512, temperature=0.7):
    """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Returns:
        tuple: (response, stats_dict)
            - response: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
            - stats_dict: í†µê³„ ì •ë³´ (ìƒì„± ì‹œê°„, í† í° ìˆ˜ ë“±)
    """
    # Chat í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    # í† í¬ë‚˜ì´ì €ì˜ chat í…œí”Œë¦¿ ì ìš©
    formatted_prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    input_token_count = inputs['input_ids'].shape[1]
    
    # ìƒì„± ì‹œì‘ ì‹œê°„
    generation_start = time.time()
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ìƒì„± ì™„ë£Œ ì‹œê°„
    generation_time = time.time() - generation_start
    
    # ë””ì½”ë”©
    input_length = inputs['input_ids'].shape[1]
    generated_tokens = outputs[0][input_length:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # ìƒì„±ëœ í† í° ìˆ˜
    generated_token_count = len(generated_tokens)
    total_token_count = outputs[0].shape[0]
    
    # í†µê³„ ì •ë³´
    stats = {
        "generation_time": generation_time,
        "input_tokens": input_token_count,
        "generated_tokens": generated_token_count,
        "total_tokens": total_token_count,
        "tokens_per_second": generated_token_count / generation_time if generation_time > 0 else 0
    }
    
    return response.strip(), stats

def interactive_chat(model_dir=None):
    """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ (ì„œë²„ ëª¨ë“œ)"""
    global _model, _tokenizer
    
    print("=" * 50)
    print("Phi-4 4BIT ì–‘ìí™” ëª¨ë¸ - ì„œë²„ ëª¨ë“œ")
    print("=" * 50)
    print("ëª¨ë¸ì„ í•œ ë²ˆ ë¡œë“œí•œ í›„ ë©”ëª¨ë¦¬ì— ìœ ì§€í•©ë‹ˆë‹¤.")
    print("=" * 50)
    print()
    
    # ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
    model, tokenizer = load_quantized_model(model_dir)
    
    print("\n" + "=" * 50)
    print("ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("=" * 50)
    print()
    
    conversation_history = []
    
    while True:
        try:
            # ë©€í‹°ë¼ì¸ ì…ë ¥ ë°›ê¸°
            print("ì‚¬ìš©ì: ", end="", flush=True)
            lines = []
            
            while True:
                try:
                    line = input()
                    # ë¹ˆ ì¤„ë„ í¬í•¨ (ì…ë ¥ì˜ ì¼ë¶€ë¡œ ê°„ì£¼)
                    lines.append(line)
                    # Ctrl+D (EOF) ë˜ëŠ” Ctrl+Cë¡œ ì…ë ¥ ì¢…ë£Œ
                except EOFError:
                    # Ctrl+Dë¡œ ì…ë ¥ ì¢…ë£Œ
                    break
                except KeyboardInterrupt:
                    # Ctrl+Cë¡œ ì·¨ì†Œ
                    print("\nì…ë ¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")
                    lines = []
                    break
            
            user_input = "\n".join(lines).strip()
            
            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not user_input:
                continue
            
            # ì¢…ë£Œ ëª…ë ¹ì–´ í™•ì¸
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            conversation_history.append({"role": "user", "content": user_input})
            
            print("\nì‘ë‹µ ìƒì„± ì¤‘...")
            
            # ì‘ë‹µ ìƒì„±
            response, stats = generate_response(
                model, 
                tokenizer, 
                user_input,
                max_new_tokens=512,
                temperature=0.7
            )
            
            # ì‘ë‹µ ì¶œë ¥
            print(f"\nPhi-4: {response}")
            print("\nğŸ“Š ìƒì„± í†µê³„:")
            print(f"  ìƒì„± ì‹œê°„: {stats['generation_time']:.2f}ì´ˆ")
            print(f"  ì…ë ¥ í† í° ìˆ˜: {stats['input_tokens']}")
            print(f"  ìƒì„±ëœ í† í° ìˆ˜: {stats['generated_tokens']}")
            print(f"  ì´ í† í° ìˆ˜: {stats['total_tokens']}")
            print(f"  ìƒì„± ì†ë„: {stats['tokens_per_second']:.2f} í† í°/ì´ˆ\n")
            
            # ëŒ€í™” ê¸°ë¡ì— ì‘ë‹µ ì¶”ê°€
            conversation_history.append({"role": "assistant", "content": response})
            
        except KeyboardInterrupt:
            print("\n\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...\n")

def single_prompt(model_dir=None):
    """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ - ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ì— ì‘ì„±ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"""
    # ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    prompt = SCRIPT_PROMPT.strip()
    
    if not prompt or prompt == "ë‹¹ì‹ ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.\nì˜ˆ: \"Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”\"":
        print("=" * 50)
        print("[WARNING] í”„ë¡¬í”„íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("=" * 50)
        print("phi4_server.py íŒŒì¼ì˜ SCRIPT_PROMPT ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬")
        print("ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.")
        print("=" * 50)
        sys.exit(1)
    
    print("=" * 50)
    print("Phi-4 4BIT ì–‘ìí™” ëª¨ë¸ - ì„œë²„ ëª¨ë“œ")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_quantized_model(model_dir)
    
    print(f"\ní”„ë¡¬í”„íŠ¸: {prompt}\n")
    print("ì‘ë‹µ ìƒì„± ì¤‘...\n")
    
    response, stats = generate_response(model, tokenizer, prompt)
    
    print("=" * 50)
    print("ì‘ë‹µ:")
    print("=" * 50)
    print(response)
    print("=" * 50)
    print("\nğŸ“Š ìƒì„± í†µê³„:")
    print(f"  ìƒì„± ì‹œê°„: {stats['generation_time']:.2f}ì´ˆ")
    print(f"  ì…ë ¥ í† í° ìˆ˜: {stats['input_tokens']}")
    print(f"  ìƒì„±ëœ í† í° ìˆ˜: {stats['generated_tokens']}")
    print(f"  ì´ í† í° ìˆ˜: {stats['total_tokens']}")
    print(f"  ìƒì„± ì†ë„: {stats['tokens_per_second']:.2f} í† í°/ì´ˆ")
    print("=" * 50)
    print()
    print("ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ìœ ì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    print("ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ë¹ ë¥´ê²Œ ì‘ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def run_api_server(port=8000, host="0.0.0.0"):
    """API ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰
    
    Args:
        port: í¬íŠ¸ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 8000)
        host: ì„œë²„ ì£¼ì†Œ (ê¸°ë³¸ê°’: '0.0.0.0')
    """
    try:
        from model_api_server import app
        import uvicorn
        
        print("=" * 50)
        print("Phi-4 API ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰")
        print("=" * 50)
        print(f"ì„œë²„ ì£¼ì†Œ: {host}:{port}")
        print("=" * 50)
        
        uvicorn.run(app, host=host, port=port)
    except ImportError:
        print("=" * 50)
        print("ì˜¤ë¥˜: model_api_server ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("=" * 50)
        print("model_api_server.py íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        print(f"API ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Phi-4 ì„œë²„ ëª¨ë“œ ì‹¤í–‰")
    parser.add_argument("--interactive", "-i", action="store_true", help="ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰")
    parser.add_argument("--model-dir", type=str, help="ë¡œì»¬ ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--api-server", action="store_true", help="API ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰")
    parser.add_argument("--port", type=int, default=8000, help="API ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 8000)")
    
    args = parser.parse_args()
    
    # API ì„œë²„ ëª¨ë“œ
    if args.api_server:
        run_api_server(port=args.port)
    # --interactive í”Œë˜ê·¸ê°€ ìˆê±°ë‚˜ ì¸ìê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ
    elif args.interactive or len(sys.argv) == 1:
        interactive_chat(args.model_dir)
    else:
        # ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ (ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        print("=" * 50)
        print("Phi-4 4BIT ì–‘ìí™” ëª¨ë¸ - ì„œë²„ ëª¨ë“œ")
        print("=" * 50)
        
        model, tokenizer = load_quantized_model(args.model_dir)
        single_prompt(args.model_dir)

