"""
Phi-4 4BIT ì–‘ìí™” ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
bitsandbytesë¥¼ ì‚¬ìš©í•˜ì—¬ 4BIT ì–‘ìí™”ëœ phi-4 ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
ë¡œì»¬ì— ì €ì¥ëœ ì–‘ìí™” ëª¨ë¸ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import sys
import os
import argparse
import time
from device_utils import check_and_setup_device, get_device_info

def load_quantized_model(model_dir=None):
    """4BIT ì–‘ìí™”ëœ phi-4 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        model_dir: ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ë˜ëŠ” Hugging Faceì—ì„œ ë¡œë“œ)
    """
    # GPU/CPU í™•ì¸ ë° ì„¤ì •
    device, device_name, is_gpu = check_and_setup_device()
    
    # ê¸°ë³¸ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
    default_local_dir = "./models/phi4-quantized"
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²°ì •
    if model_dir is None:
        # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
        if len(sys.argv) > 1 and sys.argv[1].startswith("--model-dir"):
            if "=" in sys.argv[1]:
                model_dir = sys.argv[1].split("=", 1)[1]
            elif len(sys.argv) > 2:
                model_dir = sys.argv[2]
        # ê¸°ë³¸ ë¡œì»¬ ê²½ë¡œ í™•ì¸
        elif os.path.exists(default_local_dir) and os.path.isdir(default_local_dir):
            model_dir = default_local_dir
    
    model_name = "microsoft/phi-4"
    
    # 4BIT ì–‘ìí™” ì„¤ì •
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )
    
    # ë¡œì»¬ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    if model_dir and os.path.exists(model_dir):
        print(f"ë¡œì»¬ ì €ì¥ëœ ëª¨ë¸ ë¡œë”© ì¤‘: {model_dir}")
        print("(ì–‘ìí™”ëœ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ë¡œë“œí•©ë‹ˆë‹¤)")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir,
            trust_remote_code=True
        )
        
        # pad_token ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
        # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ cuda ì‚¬ìš©
        if torch.cuda.is_available():
            print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            print("GPUì— ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            quantization_config=quantization_config,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        
        print("ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
        device_info = get_device_info(model)
        print(f"\nëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device_info['device_name']} ({device_info['device']})")
        if device_info['is_gpu']:
            print("âœ… GPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
        else:
            print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (GPU ì‚¬ìš© ê¶Œì¥)")
        
        return model, tokenizer
    
    # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ Hugging Faceì—ì„œ ë¡œë“œ
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    print("4BIT ì–‘ìí™” ì„¤ì • ì ìš© ì¤‘...")
    print("(ë¡œì»¬ ì €ì¥ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´: python save_quantized_model.py)")
    
    # Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ì„ íƒì‚¬í•­)
    cache_dir = os.getenv("HF_HOME", os.path.join(os.path.expanduser("~"), ".cache", "huggingface"))
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, 
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    
    # pad_token ì„¤ì • (ì—†ëŠ” ê²½ìš° eos_token ì‚¬ìš©)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ (ìµœì í™” ì˜µì…˜ ì¶”ê°€)
    # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ cuda ì‚¬ìš©
    if torch.cuda.is_available():
        print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print("GPUì— ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        low_cpu_mem_usage=True,
        cache_dir=cache_dir
    )
    
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸
    device_info = get_device_info(model)
    print(f"\nëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device_info['device_name']} ({device_info['device']})")
    if device_info['is_gpu']:
        print("âœ… GPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
    else:
        print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (GPU ì‚¬ìš© ê¶Œì¥)")
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_new_tokens=512, temperature=0.7):
    """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Returns:
        tuple: (response, stats_dict)
            - response: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
            - stats_dict: í†µê³„ ì •ë³´ (ìƒì„± ì‹œê°„, í† í° ìˆ˜ ë“±)
    """
    # Chat í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    # í† í¬ë‚˜ì´ì €ì˜ chat í…œí”Œë¦¿ ì ìš©
    formatted_prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    input_token_count = inputs['input_ids'].shape[1]
    
    # ìƒì„± ì‹œì‘ ì‹œê°„
    generation_start = time.time()
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ìƒì„± ì™„ë£Œ ì‹œê°„
    generation_time = time.time() - generation_start
    
    # ë””ì½”ë”©
    # ì…ë ¥ ê¸¸ì´ë§Œí¼ ì œì™¸í•˜ê³  ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©
    input_length = inputs['input_ids'].shape[1]
    generated_tokens = outputs[0][input_length:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # ìƒì„±ëœ í† í° ìˆ˜
    generated_token_count = len(generated_tokens)
    total_token_count = outputs[0].shape[0]
    
    # í†µê³„ ì •ë³´
    stats = {
        "generation_time": generation_time,
        "input_tokens": input_token_count,
        "generated_tokens": generated_token_count,
        "total_tokens": total_token_count,
        "tokens_per_second": generated_token_count / generation_time if generation_time > 0 else 0
    }
    
    return response.strip(), stats

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Phi-4 4BIT ì–‘ìí™” ëª¨ë¸ ì‹¤í–‰")
    parser.add_argument("prompt", nargs="?", help="ì…ë ¥ í”„ë¡¬í”„íŠ¸")
    parser.add_argument("--model-dir", type=str, help="ë¡œì»¬ ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    if not args.prompt:
        print("ì‚¬ìš©ë²•: python phi4_quantized.py <í”„ë¡¬í”„íŠ¸> [--model-dir <ê²½ë¡œ>]")
        print("ë˜ëŠ”: python prompt_input.py ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”í˜•ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)
    
    prompt = args.prompt
    
    print("=" * 50)
    print("Phi-4 4BIT ì–‘ìí™” ëª¨ë¸ ì‹¤í–‰")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_quantized_model()
    
    print(f"\ní”„ë¡¬í”„íŠ¸: {prompt}\n")
    print("ì‘ë‹µ ìƒì„± ì¤‘...\n")
    
    # ì‘ë‹µ ìƒì„±
    response, stats = generate_response(model, tokenizer, prompt)
    
    print("=" * 50)
    print("ì‘ë‹µ:")
    print("=" * 50)
    print(response)
    print("=" * 50)
    print("\nğŸ“Š ìƒì„± í†µê³„:")
    print(f"  ìƒì„± ì‹œê°„: {stats['generation_time']:.2f}ì´ˆ")
    print(f"  ì…ë ¥ í† í° ìˆ˜: {stats['input_tokens']}")
    print(f"  ìƒì„±ëœ í† í° ìˆ˜: {stats['generated_tokens']}")
    print(f"  ì´ í† í° ìˆ˜: {stats['total_tokens']}")
    print(f"  ìƒì„± ì†ë„: {stats['tokens_per_second']:.2f} í† í°/ì´ˆ")
    print("=" * 50)

if __name__ == "__main__":
    main()

