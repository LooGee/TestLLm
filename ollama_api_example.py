"""
Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸
Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë“¤ê³¼ ë™ì¼í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import requests
import json
import sys
import argparse
import time
import re

# ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ì— ì‘ì„±í•  í”„ë¡¬í”„íŠ¸
# ì´ ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì •í•˜ì„¸ìš”
SCRIPT_PROMPT = """
ë‹¹ì‹ ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.
ì˜ˆ: "Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”"
"""

def extract_json_from_response(response_text):
    """
    ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json ... ```)ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        response_text: ëª¨ë¸ ì‘ë‹µ í…ìŠ¤íŠ¸
    
    Returns:
        str: ì¶”ì¶œëœ JSON ë¬¸ìì—´ (ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜)
    """
    if not response_text:
        return response_text
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ (```json ... ``` ë˜ëŠ” ``` ... ```)
    json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
    match = re.search(json_block_pattern, response_text, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
        # ìœ íš¨í•œ JSONì¸ì§€ í™•ì¸
        try:
            json.loads(json_str)
            return json_str
        except json.JSONDecodeError:
            pass
    
    # 2. ì²« ë²ˆì§¸ { ë¶€í„° ì‹œì‘í•˜ì—¬ ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ì™„ì „í•œ JSON ê°ì²´ ì¶”ì¶œ
    start_idx = response_text.find('{')
    if start_idx != -1:
        brace_count = 0
        end_idx = start_idx
        
        for i in range(start_idx, len(response_text)):
            if response_text[i] == '{':
                brace_count += 1
            elif response_text[i] == '}':
                brace_count -= 1
                if brace_count == 0:
                    end_idx = i + 1
                    break
        
        if brace_count == 0:
            json_str = response_text[start_idx:end_idx].strip()
            # ìœ íš¨í•œ JSONì¸ì§€ í™•ì¸
            try:
                json.loads(json_str)
                return json_str
            except json.JSONDecodeError:
                pass
    
    # 3. JSONì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    return response_text

def check_ollama_installed():
    """
    Ollamaê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Returns:
        bool: Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ True
    """
    import subprocess
    import shutil
    
    # ollama ëª…ë ¹ì–´ê°€ PATHì— ìˆëŠ”ì§€ í™•ì¸
    if shutil.which("ollama"):
        return True
    
    # Windowsì—ì„œ ì¼ë°˜ì ì¸ ì„¤ì¹˜ ê²½ë¡œ í™•ì¸
    import os
    if os.name == 'nt':  # Windows
        common_paths = [
            os.path.expanduser("~\\AppData\\Local\\Programs\\Ollama\\ollama.exe"),
            "C:\\Program Files\\Ollama\\ollama.exe",
        ]
        for path in common_paths:
            if os.path.exists(path):
                return True
    
    return False

def check_ollama_server(base_url="http://localhost:11434"):
    """
    Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        base_url: Ollama ì„œë²„ URL
    
    Returns:
        tuple: (is_available, models_list)
            - is_available: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ True
            - models_list: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    """
    # ë¨¼ì € Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if not check_ollama_installed():
        return False, []
    
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        if response.status_code == 200:
            result = response.json()
            models = [model.get("name", "") for model in result.get("models", [])]
            return True, models
        return False, []
    except:
        return False, []

def check_model_exists(model, base_url="http://localhost:11434"):
    """
    íŠ¹ì • ëª¨ë¸ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        model: í™•ì¸í•  ëª¨ë¸ ì´ë¦„
        base_url: Ollama ì„œë²„ URL
    
    Returns:
        bool: ëª¨ë¸ì´ ì¡´ì¬í•˜ë©´ True
    """
    is_available, models = check_ollama_server(base_url)
    if not is_available:
        return False
    
    # ëª¨ë¸ ì´ë¦„ ë§¤ì¹­ (ì •í™•í•œ ì´ë¦„ ë˜ëŠ” ì ‘ë‘ì‚¬)
    for available_model in models:
        if model == available_model or available_model.startswith(model + ":"):
            return True
    return False

def query_ollama(prompt, model="phi4-quantized", base_url="http://localhost:11434", system_prompt=None):
    """
    Ollama APIë¥¼ í†µí•´ ëª¨ë¸ì— ì¿¼ë¦¬ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
    
    Args:
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: phi4-quantized)
        base_url: Ollama ì„œë²„ URL
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        tuple: (response, stats_dict)
            - response: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
            - stats_dict: í†µê³„ ì •ë³´ (ìƒì„± ì‹œê°„, í† í° ìˆ˜ ë“±)
    """
    # Ollama ì„¤ì¹˜ í™•ì¸
    if not check_ollama_installed():
        print("=" * 50)
        print("âš ï¸  Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("=" * 50)
        print("Ollama ì„¤ì¹˜ ë°©ë²•:")
        print("1. Windows: https://ollama.com/ ì—ì„œ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜")
        print("2. Linux: curl -fsSL https://ollama.com/install.sh | sh")
        print("3. Mac: brew install ollama")
        print("\nì„¤ì¹˜ í›„ Ollama ì•±ì„ ì‹¤í–‰í•˜ê±°ë‚˜ 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        print("=" * 50)
        return None, None
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    is_available, models = check_ollama_server(base_url)
    if not is_available:
        print("=" * 50)
        print("âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("=" * 50)
        print(f"ì„œë²„ URL: {base_url}")
        print("\ní™•ì¸ ì‚¬í•­:")
        print("1. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")
        print("   - Windows: Ollama ì•±ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (ì‹œì‘ ë©”ë‰´ì—ì„œ 'Ollama' ê²€ìƒ‰)")
        print("   - Linux/Mac: 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘")
        print("2. ì„œë²„ URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸ (ê¸°ë³¸ê°’: http://localhost:11434)")
        print("3. ë°©í™”ë²½ì´ í¬íŠ¸ 11434ë¥¼ ì°¨ë‹¨í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸")
        print("=" * 50)
        return None, None
    
    # ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not check_model_exists(model, base_url):
        print("=" * 50)
        print(f"âš ï¸  ëª¨ë¸ '{model}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("=" * 50)
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
        if models:
            for m in models:
                print(f"  - {m}")
        else:
            print("  (ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤)")
        print(f"\nëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´:")
        print(f"  ollama pull {model}")
        print("=" * 50)
        return None, None
    
    url = f"{base_url}/api/chat"
    
    # messages ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    # promptê°€ ë¬¸ìì—´ì´ë©´ user ë©”ì‹œì§€ë¡œ ì¶”ê°€
    if isinstance(prompt, str):
        messages.append({"role": "user", "content": prompt})
    else:
        # promptê°€ ì´ë¯¸ messages ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (í–¥í›„ í™•ì¥ì„±)
        messages.extend(prompt)
    
    data = {
        "model": model,
        "messages": messages,
        "stream": False
    }
    
    try:
        generation_start = time.time()
        response = requests.post(url, json=data, timeout=300)
        response.raise_for_status()
        result = response.json()
        generation_time = time.time() - generation_start
        
        # /api/chat ì‘ë‹µ í˜•ì‹: {"message": {"role": "assistant", "content": "..."}}
        message = result.get("message", {})
        response_text = message.get("content", "")
        
        # JSONë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°)
        response_text = extract_json_from_response(response_text)
        
        # í†µê³„ ì •ë³´ ì¶”ì¶œ (/api/chat ì‘ë‹µ í˜•ì‹)
        stats = {
            "generation_time": generation_time,
            "total_duration": result.get("total_duration", 0) / 1e9 if result.get("total_duration") else 0,  # ë‚˜ë…¸ì´ˆë¥¼ ì´ˆë¡œ ë³€í™˜
            "load_duration": result.get("load_duration", 0) / 1e9 if result.get("load_duration") else 0,
            "prompt_eval_count": result.get("prompt_eval_count", 0),
            "eval_count": result.get("eval_count", 0),
            "total_tokens": result.get("prompt_eval_count", 0) + result.get("eval_count", 0),
            "tokens_per_second": result.get("eval_count", 0) / generation_time if generation_time > 0 and result.get("eval_count") else 0
        }
        
        return response_text, stats
    except requests.exceptions.ConnectionError as e:
        print("=" * 50)
        print("âš ï¸  Ollama ì„œë²„ ì—°ê²° ì˜¤ë¥˜")
        print("=" * 50)
        print(f"ì„œë²„ URL: {base_url}")
        print(f"ì˜¤ë¥˜: {e}")
        print("\nOllama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print("1. Ollama ì•±ì„ ì‹¤í–‰í•˜ì„¸ìš”")
        print("2. ë˜ëŠ” í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        print("=" * 50)
        return None, None
    except requests.exceptions.RequestException as e:
        print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return None, None

def interactive_chat(model="phi4-quantized", base_url="http://localhost:11434", system_prompt=None):
    """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    print("=" * 50)
    print(f"Ollama API - {model} ëª¨ë¸")
    print("=" * 50)
    
    # Ollama ì„¤ì¹˜ í™•ì¸
    if not check_ollama_installed():
        print("=" * 50)
        print("âš ï¸  Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("=" * 50)
        print("Ollama ì„¤ì¹˜ ë°©ë²•:")
        print("1. Windows: https://ollama.com/ ì—ì„œ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜")
        print("2. Linux: curl -fsSL https://ollama.com/install.sh | sh")
        print("3. Mac: brew install ollama")
        print("\nì„¤ì¹˜ í›„ Ollama ì•±ì„ ì‹¤í–‰í•˜ê±°ë‚˜ 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        print("=" * 50)
        return
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    print("Ollama ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘...")
    is_available, models = check_ollama_server(base_url)
    if not is_available:
        print("âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì„œë²„ URL: {base_url}")
        print("\nOllama ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
        print("1. Windows: ì‹œì‘ ë©”ë‰´ì—ì„œ 'Ollama' ì•±ì„ ì‹¤í–‰í•˜ì„¸ìš”")
        print("2. Linux/Mac: í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        print("\nì„œë²„ê°€ ì‹¤í–‰ë˜ë©´ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return
    else:
        print("âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ!")
        if models:
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(models[:5])}" + (f" ì™¸ {len(models)-5}ê°œ" if len(models) > 5 else ""))
        
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸
        if not check_model_exists(model, base_url):
            print(f"\nâš ï¸  ëª¨ë¸ '{model}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
            for m in models:
                print(f"  - {m}")
            print(f"\nëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´:")
            print(f"  ollama pull {model}")
            return
    
    print("\nëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ë©€í‹°ë¼ì¸ ì…ë ¥: ì—¬ëŸ¬ ì¤„ ì…ë ¥ í›„ Ctrl+D (Windows: Ctrl+Z í›„ Enter)ë¡œ ì™„ë£Œ")
    if system_prompt:
        print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt[:50]}...")
    print("=" * 50)
    print()
    
    conversation_history = []
    
    while True:
        try:
            # ë©€í‹°ë¼ì¸ ì…ë ¥ ë°›ê¸°
            print("ì‚¬ìš©ì: ", end="", flush=True)
            lines = []
            
            while True:
                try:
                    line = input()
                    # ë¹ˆ ì¤„ë„ í¬í•¨ (ì…ë ¥ì˜ ì¼ë¶€ë¡œ ê°„ì£¼)
                    lines.append(line)
                    # Ctrl+D (EOF) ë˜ëŠ” Ctrl+Cë¡œ ì…ë ¥ ì¢…ë£Œ
                except EOFError:
                    # Ctrl+Dë¡œ ì…ë ¥ ì¢…ë£Œ
                    break
                except KeyboardInterrupt:
                    # Ctrl+Cë¡œ ì·¨ì†Œ
                    print("\nì…ë ¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")
                    lines = []
                    break
            
            user_input = "\n".join(lines).strip()
            
            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not user_input:
                continue
            
            # ì¢…ë£Œ ëª…ë ¹ì–´ í™•ì¸
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            conversation_history.append({"role": "user", "content": user_input})
            
            print("\nì‘ë‹µ ìƒì„± ì¤‘...")
            
            # ì‘ë‹µ ìƒì„±
            response, stats = query_ollama(user_input, model=model, base_url=base_url, system_prompt=system_prompt)
            
            if response and stats:
                print(f"\n{model}: {response}")
                print("\nğŸ“Š ìƒì„± í†µê³„:")
                print(f"  ìƒì„± ì‹œê°„: {stats['generation_time']:.2f}ì´ˆ")
                print(f"  í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {stats['prompt_eval_count']}")
                print(f"  ìƒì„±ëœ í† í° ìˆ˜: {stats['eval_count']}")
                print(f"  ì´ í† í° ìˆ˜: {stats['total_tokens']}")
                print(f"  ìƒì„± ì†ë„: {stats['tokens_per_second']:.2f} í† í°/ì´ˆ")
                print()
                
                # ëŒ€í™” ê¸°ë¡ì— ì‘ë‹µ ì¶”ê°€
                conversation_history.append({"role": "assistant", "content": response})
            else:
                print("\nì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\n")
            
        except KeyboardInterrupt:
            print("\n\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...\n")

def single_prompt(model="phi4-quantized", base_url="http://localhost:11434", system_prompt=None):
    """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ - ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ì— ì‘ì„±ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"""
    # ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    prompt = SCRIPT_PROMPT.strip()
    
    if not prompt or prompt == "ë‹¹ì‹ ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.\nì˜ˆ: \"Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”\"":
        print("=" * 50)
        print("âš ï¸  í”„ë¡¬í”„íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("=" * 50)
        print("ollama_api_example.py íŒŒì¼ì˜ SCRIPT_PROMPT ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬")
        print("ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.")
        print("=" * 50)
        sys.exit(1)
    
    print("=" * 50)
    print(f"Ollama API - {model} ëª¨ë¸")
    print("=" * 50)
    
    # Ollama ì„¤ì¹˜ í™•ì¸
    if not check_ollama_installed():
        print("=" * 50)
        print("âš ï¸  Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("=" * 50)
        print("Ollama ì„¤ì¹˜ ë°©ë²•:")
        print("1. Windows: https://ollama.com/ ì—ì„œ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜")
        print("2. Linux: curl -fsSL https://ollama.com/install.sh | sh")
        print("3. Mac: brew install ollama")
        print("\nì„¤ì¹˜ í›„ Ollama ì•±ì„ ì‹¤í–‰í•˜ê±°ë‚˜ 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        print("=" * 50)
        sys.exit(1)
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    print("Ollama ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘...")
    is_available, models = check_ollama_server(base_url)
    if not is_available:
        print("âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì„œë²„ URL: {base_url}")
        print("\nOllama ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
        print("1. Windows: ì‹œì‘ ë©”ë‰´ì—ì„œ 'Ollama' ì•±ì„ ì‹¤í–‰í•˜ì„¸ìš”")
        print("2. Linux/Mac: í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        sys.exit(1)
    else:
        print("âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ!")
        if models:
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(models[:5])}" + (f" ì™¸ {len(models)-5}ê°œ" if len(models) > 5 else ""))
        
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸
        if not check_model_exists(model, base_url):
            print(f"\nâš ï¸  ëª¨ë¸ '{model}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
            for m in models:
                print(f"  - {m}")
            print(f"\nëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´:")
            print(f"  ollama pull {model}")
            sys.exit(1)
    
    print(f"\ní”„ë¡¬í”„íŠ¸: {prompt}\n")
    if system_prompt:
        print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}\n")
    print("ì‘ë‹µ ìƒì„± ì¤‘...\n")
    
    response, stats = query_ollama(prompt, model=model, base_url=base_url, system_prompt=system_prompt)
    
    if response and stats:
        print("=" * 50)
        print("ì‘ë‹µ:")
        print("=" * 50)
        print(response)
        print("=" * 50)
        print("\nğŸ“Š ìƒì„± í†µê³„:")
        print(f"  ìƒì„± ì‹œê°„: {stats['generation_time']:.2f}ì´ˆ")
        print(f"  í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜: {stats['prompt_eval_count']}")
        print(f"  ìƒì„±ëœ í† í° ìˆ˜: {stats['eval_count']}")
        print(f"  ì´ í† í° ìˆ˜: {stats['total_tokens']}")
        print(f"  ìƒì„± ì†ë„: {stats['tokens_per_second']:.2f} í† í°/ì´ˆ")
        print("=" * 50)
    else:
        print("ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Ollama API ì‹¤í–‰")
    parser.add_argument("--interactive", "-i", action="store_true", help="ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰")
    parser.add_argument("--model", "-m", type=str, default="phi4-quantized", help="ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: phi4-quantized)")
    parser.add_argument("--base-url", type=str, default="http://localhost:11434", help="Ollama ì„œë²„ URL")
    parser.add_argument("--system-prompt", "-s", type=str, default=None, help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •")
    
    args = parser.parse_args()
    
    # --interactive í”Œë˜ê·¸ê°€ ìˆê±°ë‚˜ ì¸ìê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ
    if args.interactive or len(sys.argv) == 1:
        interactive_chat(model=args.model, base_url=args.base_url, system_prompt=args.system_prompt)
    else:
        # ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ (ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        single_prompt(model=args.model, base_url=args.base_url, system_prompt=args.system_prompt)

if __name__ == "__main__":
    # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        import requests
    except ImportError:
        print("requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install requests")
        sys.exit(1)
    
    main()

